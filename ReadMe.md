# ğŸ“Š Evaluating AI Models for Student Competence Analysis

This repository evaluates the potential of **AI models** for analyzing student-written Python code and assessing competence. The objective is to explore how different models can:
- Analyze and interpret student Python code
- Generate prompts to test conceptual understanding
- Identify reasoning gaps or misconceptions
- Encourage deeper learning without directly providing solutions



## ğŸ” Models Evaluated

| Model | Type | Key Strengths | Limitations |
|-------|------|---------------|-------------|
| **ChatGPT (OpenAI)** | Proprietary LLM | Strong reasoning, reliable coding feedback, high-quality prompts | Paid API, not open-source |
| **Gemini (Google DeepMind)** | Proprietary LLM | Multimodal (text + code), integrates with Google ecosystem | Early-stage API, limited interpretability |
| **Claude (Anthropic)** | Proprietary LLM | Safe, structured explanations, strong reasoning alignment | Context window constraints |
| **Perplexity AI** | AI Search + LLM | Real-time retrieval, contextual knowledge integration | Less code-specialized, web-reliant |
| **StarCoder (BigCode Project)** | **Open-Source LLM** | Code-focused, trained on GitHub repositories, customizable & free | Requires fine-tuning for educational use |


## ğŸ› ï¸ Research Plan

**Approach**
- Use student-written Python code samples as input.
- Evaluate model outputs for correctness, reasoning, and educational guidance.
- Compare how prompts encourage exploration rather than giving direct answers.

**Evaluation Criteria**
- **Accuracy**: Detection of errors and misconceptions.
- **Educational Value**: Prompts that foster deeper understanding.
- **Interpretability**: Transparency in reasoning.
- **Cost & Accessibility**: Free vs proprietary APIs.

**Testing**
- Prepare Python scripts with common student mistakes.
- Query each model for feedback.
- Collect structured metrics (accuracy, quality of prompts, cost-efficiency).



## âš–ï¸ Reasoning

- **Suitability**: The best model should explain *why* mistakes occur, not just correct them.
- **Meaningful Prompts**: Prompts should guide reflection (e.g., â€œWhat happens if the loop runs one more time?â€).
- **Trade-offs**:
Â  - Proprietary models â†’ higher accuracy, less transparency, paid.
Â  - Open-source (StarCoder) â†’ customizable, transparent, free, but needs tuning.



## ğŸ“‚ Folder Structure

student-competence-analysis/
â”‚â”€â”€ data/ # Sample student Python code
â”‚â”€â”€ notebooks/ # Jupyter notebooks for experiments
â”‚â”€â”€ src/ # Scripts for each model evaluation
â”‚ â”œâ”€â”€ chatgpt_eval.py
â”‚ â”œâ”€â”€ gemini_eval.py
â”‚ â”œâ”€â”€ claude_eval.py
â”‚ â”œâ”€â”€ perplexity_eval.py
â”‚ â”œâ”€â”€ starcoder_eval.py
â”‚â”€â”€ results/ # Outputs & analysis reports
â”‚â”€â”€ README.md # Project overview



## ğŸš€ Setup & Usage

1. **Clone the Repository**
Â   ```bash
Â   git clone https://github.com/anshikagarg12/Task3_Fossee
Â   cd Task3_Fossee

2. Install Dependencies

pip install -r requirements.txt

3. Run Model Evaluations

python src/chatgpt_eval.py
python src/gemini_eval.py
python src/claude_eval.py
python src/perplexity_eval.py
python src/starcoder_eval.py

ğŸ“Œ References

OpenAI ChatGPT

Google Gemini

Anthropic Claude

Perplexity AI

StarCoder (BigCode Project)